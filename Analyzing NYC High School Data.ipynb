{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Analyzing NYC High School Data\n",
    " ---\n",
    " This is a guided project part of the Dataquest.io courses. The goal of this project is to examine NYC high school data to find out what causes high SAT scores.\n",
    "\n",
    " There is a good amount of public data available regarding education statistics and SAT scores. The only issue is that these are available in separate sources. Therefore the first part of this project we will need to read in the data and clean it up enough so that all of the datasets can be combined.\n",
    "\n",
    " Additionally, we will also experiment with using the Altair library for visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib\n",
    "import altair as alt\n",
    "import geopandas as gpd\n",
    "import gpdvega\n",
    "\n",
    "data_files = [\n",
    "    \"ap_2010.csv\",\n",
    "    \"class_size.csv\",\n",
    "    \"demographics.csv\",\n",
    "    \"graduation.csv\",\n",
    "    \"hs_directory.csv\",\n",
    "    \"sat_results.csv\"\n",
    "]\n",
    "\n",
    "data = {}\n",
    "\n",
    "for f in data_files:\n",
    "    d = pd.read_csv(\"data/schools/{0}\".format(f))\n",
    "    data[f.replace(\".csv\", \"\")] = d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_survey = pd.read_csv(\"data/schools/survey_all.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "d75_survey = pd.read_csv(\"data/schools/survey_d75.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "survey = pd.concat([all_survey, d75_survey], axis=0, sort=False)\n",
    "\n",
    "survey[\"DBN\"] = survey[\"dbn\"]\n",
    "\n",
    "survey_fields = [\n",
    "    \"DBN\", \n",
    "    \"rr_s\", \n",
    "    \"rr_t\", \n",
    "    \"rr_p\", \n",
    "    \"N_s\", \n",
    "    \"N_t\", \n",
    "    \"N_p\", \n",
    "    \"saf_p_11\", \n",
    "    \"com_p_11\", \n",
    "    \"eng_p_11\", \n",
    "    \"aca_p_11\", \n",
    "    \"saf_t_11\", \n",
    "    \"com_t_11\", \n",
    "    \"eng_t_11\", \n",
    "    \"aca_t_11\", \n",
    "    \"saf_s_11\", \n",
    "    \"com_s_11\", \n",
    "    \"eng_s_11\", \n",
    "    \"aca_s_11\", \n",
    "    \"saf_tot_11\", \n",
    "    \"com_tot_11\", \n",
    "    \"eng_tot_11\", \n",
    "    \"aca_tot_11\",\n",
    "]\n",
    "survey = survey.loc[:,survey_fields]\n",
    "data[\"survey\"] = survey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DBN', 'rr_s', 'rr_t', 'rr_p', 'N_s', 'N_t', 'N_p', 'saf_p_11',\n",
       "       'com_p_11', 'eng_p_11', 'aca_p_11', 'saf_t_11', 'com_t_11', 'eng_t_11',\n",
       "       'aca_t_11', 'saf_s_11', 'com_s_11', 'eng_s_11', 'aca_s_11',\n",
       "       'saf_tot_11', 'com_tot_11', 'eng_tot_11', 'aca_tot_11'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_survey = pd.read_csv(\"data/schools/survey_all.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "d75_survey = pd.read_csv(\"data/schools/survey_d75.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "survey = pd.concat([all_survey, d75_survey], axis=0, sort=False)\n",
    "\n",
    "survey[\"DBN\"] = survey[\"dbn\"]\n",
    "sruvey_fields = survey.columns\n",
    "\n",
    "survey = survey.loc[:,survey_fields]\n",
    "data[\"survey\"] = survey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our datasets are not encoded in a standard format. There isn't really a way to tell except to try loading and see how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DBN', 'rr_s', 'rr_t', 'rr_p', 'N_s', 'N_t', 'N_p', 'saf_p_11',\n",
       "       'com_p_11', 'eng_p_11', 'aca_p_11', 'saf_t_11', 'com_t_11', 'eng_t_11',\n",
       "       'aca_t_11', 'saf_s_11', 'com_s_11', 'eng_s_11', 'aca_s_11',\n",
       "       'saf_tot_11', 'com_tot_11', 'eng_tot_11', 'aca_tot_11'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"survey\"].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_survey = pd.read_csv(\"data/schools/survey_all.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "d75_survey = pd.read_csv(\"data/schools/survey_d75.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "survey = pd.concat([all_survey, d75_survey], axis=0, sort=False)\n",
    "\n",
    "survey[\"DBN\"] = survey[\"dbn\"]\n",
    "sruvey_fields = survey.columns\n",
    "\n",
    "survey = survey.loc[:,survey_fields]\n",
    "data[\"survey\"] = survey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Analyzing NYC High School Data\n",
    " ---\n",
    " This is a guided project part of the Dataquest.io courses. The goal of this project is to examine NYC high school data to find out what causes high SAT scores.\n",
    "\n",
    " There is a good amount of public data available regarding education statistics and SAT scores. The only issue is that these are available in separate sources. Therefore the first part of this project we will need to read in the data and clean it up enough so that all of the datasets can be combined.\n",
    "\n",
    " Additionally, we will also experiment with using the Altair library for visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib\n",
    "import altair as alt\n",
    "import geopandas as gpd\n",
    "import gpdvega\n",
    "\n",
    "data_files = [\n",
    "    \"ap_2010.csv\",\n",
    "    \"class_size.csv\",\n",
    "    \"demographics.csv\",\n",
    "    \"graduation.csv\",\n",
    "    \"hs_directory.csv\",\n",
    "    \"sat_results.csv\"\n",
    "]\n",
    "\n",
    "data = {}\n",
    "\n",
    "for f in data_files:\n",
    "    d = pd.read_csv(\"data/schools/{0}\".format(f))\n",
    "    data[f.replace(\".csv\", \"\")] = d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our datasets are not encoded in a standard format. There isn't really a way to tell except to try loading and see how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_survey = pd.read_csv(\"data/schools/survey_all.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "d75_survey = pd.read_csv(\"data/schools/survey_d75.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "survey = pd.concat([all_survey, d75_survey], axis=0, sort=False)\n",
    "\n",
    "survey[\"DBN\"] = survey[\"dbn\"]\n",
    "sruvey_fields = survey.columns\n",
    "\n",
    "survey = survey.loc[:,survey_fields]\n",
    "data[\"survey\"] = survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When working with multiple datasets, a common field is required to join them together into one set. In many datasets, it may not be obvious what that key field is.\n",
    " Sometimes, a different key is required for each pairing of sets. We can use DBN as our key column for all sets. This will require additional cleaning though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"hs_directory\"][\"DBN\"] = data[\"hs_directory\"][\"dbn\"]\n",
    "\n",
    "def pad_csd(num):\n",
    "    string_representation = str(num)\n",
    "    if len(string_representation) > 1:\n",
    "        return string_representation\n",
    "    else:\n",
    "        return \"0\" + string_representation\n",
    "    \n",
    "data[\"class_size\"][\"padded_csd\"] = data[\"class_size\"][\"CSD\"].apply(pad_csd)\n",
    "data[\"class_size\"][\"DBN\"] = data[\"class_size\"][\"padded_csd\"] + data[\"class_size\"][\"SCHOOL CODE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['SAT Math Avg. Score', 'SAT Critical Reading Avg. Score', 'SAT Writing Avg. Score']\n",
    "for c in cols:\n",
    "    data[\"sat_results\"][c] = pd.to_numeric(data[\"sat_results\"][c], errors=\"coerce\")\n",
    "\n",
    "data['sat_results']['sat_score'] = data['sat_results'][cols[0]] + data['sat_results'][cols[1]] + data['sat_results'][cols[2]]\n",
    "\n",
    "def find_lat(loc):\n",
    "    coords = re.findall(\"\\(.+, .+\\)\", loc)\n",
    "    lat = coords[0].split(\",\")[0].replace(\"(\", \"\")\n",
    "    return lat\n",
    "\n",
    "def find_lon(loc):\n",
    "    coords = re.findall(\"\\(.+, .+\\)\", loc)\n",
    "    lon = coords[0].split(\",\")[1].replace(\")\", \"\").strip()\n",
    "    return lon\n",
    "\n",
    "data[\"hs_directory\"][\"lat\"] = data[\"hs_directory\"][\"Location 1\"].apply(find_lat)\n",
    "data[\"hs_directory\"][\"lon\"] = data[\"hs_directory\"][\"Location 1\"].apply(find_lon)\n",
    "\n",
    "data[\"hs_directory\"][\"lat\"] = pd.to_numeric(data[\"hs_directory\"][\"lat\"], errors=\"coerce\")\n",
    "data[\"hs_directory\"][\"lon\"] = pd.to_numeric(data[\"hs_directory\"][\"lon\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Analyzing NYC High School Data\n",
    " ---\n",
    " This is a guided project part of the Dataquest.io courses. The goal of this project is to examine NYC high school data to find out what causes high SAT scores.\n",
    "\n",
    " There is a good amount of public data available regarding education statistics and SAT scores. The only issue is that these are available in separate sources. Therefore the first part of this project we will need to read in the data and clean it up enough so that all of the datasets can be combined.\n",
    "\n",
    " Additionally, we will also experiment with using the Altair library for visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib\n",
    "import altair as alt\n",
    "import geopandas as gpd\n",
    "import gpdvega\n",
    "\n",
    "data_files = [\n",
    "    \"ap_2010.csv\",\n",
    "    \"class_size.csv\",\n",
    "    \"demographics.csv\",\n",
    "    \"graduation.csv\",\n",
    "    \"hs_directory.csv\",\n",
    "    \"sat_results.csv\"\n",
    "]\n",
    "\n",
    "data = {}\n",
    "\n",
    "for f in data_files:\n",
    "    d = pd.read_csv(\"data/schools/{0}\".format(f))\n",
    "    data[f.replace(\".csv\", \"\")] = d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our datasets are not encoded in a standard format. There isn't really a way to tell except to try loading and see how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_survey = pd.read_csv(\"data/schools/survey_all.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "d75_survey = pd.read_csv(\"data/schools/survey_d75.txt\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "survey = pd.concat([all_survey, d75_survey], axis=0, sort=False)\n",
    "\n",
    "survey[\"DBN\"] = survey[\"dbn\"]\n",
    "sruvey_fields = survey.columns\n",
    "\n",
    "survey = survey.loc[:,survey_fields]\n",
    "data[\"survey\"] = survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When working with multiple datasets, a common field is required to join them together into one set. In many datasets, it may not be obvious what that key field is.\n",
    " Sometimes, a different key is required for each pairing of sets. We can use DBN as our key column for all sets. This will require additional cleaning though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"hs_directory\"][\"DBN\"] = data[\"hs_directory\"][\"dbn\"]\n",
    "\n",
    "def pad_csd(num):\n",
    "    string_representation = str(num)\n",
    "    if len(string_representation) > 1:\n",
    "        return string_representation\n",
    "    else:\n",
    "        return \"0\" + string_representation\n",
    "    \n",
    "data[\"class_size\"][\"padded_csd\"] = data[\"class_size\"][\"CSD\"].apply(pad_csd)\n",
    "data[\"class_size\"][\"DBN\"] = data[\"class_size\"][\"padded_csd\"] + data[\"class_size\"][\"SCHOOL CODE\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next is to convert our numeric columns to actually be numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['SAT Math Avg. Score', 'SAT Critical Reading Avg. Score', 'SAT Writing Avg. Score']\n",
    "for c in cols:\n",
    "    data[\"sat_results\"][c] = pd.to_numeric(data[\"sat_results\"][c], errors=\"coerce\")\n",
    "\n",
    "data['sat_results']['sat_score'] = data['sat_results'][cols[0]] + data['sat_results'][cols[1]] + data['sat_results'][cols[2]]\n",
    "\n",
    "def find_lat(loc):\n",
    "    coords = re.findall(\"\\(.+, .+\\)\", loc)\n",
    "    lat = coords[0].split(\",\")[0].replace(\"(\", \"\")\n",
    "    return lat\n",
    "\n",
    "def find_lon(loc):\n",
    "    coords = re.findall(\"\\(.+, .+\\)\", loc)\n",
    "    lon = coords[0].split(\",\")[1].replace(\")\", \"\").strip()\n",
    "    return lon\n",
    "\n",
    "data[\"hs_directory\"][\"lat\"] = data[\"hs_directory\"][\"Location 1\"].apply(find_lat)\n",
    "data[\"hs_directory\"][\"lon\"] = data[\"hs_directory\"][\"Location 1\"].apply(find_lon)\n",
    "\n",
    "data[\"hs_directory\"][\"lat\"] = pd.to_numeric(data[\"hs_directory\"][\"lat\"], errors=\"coerce\")\n",
    "data[\"hs_directory\"][\"lon\"] = pd.to_numeric(data[\"hs_directory\"][\"lon\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, we'll do some some filtering and grouping to get the data in a more analysis-friendly format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size = data[\"class_size\"]\n",
    "class_size = class_size[class_size[\"GRADE \"] == \"09-12\"]\n",
    "class_size = class_size[class_size[\"PROGRAM TYPE\"] == \"GEN ED\"]\n",
    "\n",
    "class_size = class_size.groupby(\"DBN\").agg(np.mean)\n",
    "class_size.reset_index(inplace=True)\n",
    "data[\"class_size\"] = class_size\n",
    "\n",
    "data[\"demographics\"] = data[\"demographics\"][data[\"demographics\"][\"schoolyear\"] == 20112012]\n",
    "\n",
    "data[\"graduation\"] = data[\"graduation\"][data[\"graduation\"][\"Cohort\"] == \"2006\"]\n",
    "data[\"graduation\"] = data[\"graduation\"][data[\"graduation\"][\"Demographic\"] == \"Total Cohort\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['AP Test Takers ', 'Total Exams Taken', 'Number of Exams with scores 3 4 or 5']\n",
    "\n",
    "for col in cols:\n",
    "    data[\"ap_2010\"][col] = pd.to_numeric(data[\"ap_2010\"][col], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, we'll use `merge` to join the datasets together. Note that the `how` argument means that all rows from the `combined` dataframe will be retained even if a match in the `ap_2010` is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = data[\"sat_results\"]\n",
    "\n",
    "combined = combined.merge(data[\"ap_2010\"], on=\"DBN\", how=\"left\")\n",
    "combined = combined.merge(data[\"graduation\"], on=\"DBN\", how=\"left\")\n",
    "\n",
    "to_merge = [\"class_size\", \"demographics\", \"survey\", \"hs_directory\"]\n",
    "\n",
    "for m in to_merge:\n",
    "    combined = combined.merge(data[m], on=\"DBN\", how=\"inner\")\n",
    "\n",
    "combined = combined.fillna(combined.mean())\n",
    "combined = combined.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Add a school district column for mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We add a school district column as that will be helpful for when we map the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_two_chars(dbn):\n",
    "    return dbn[0:2]\n",
    "\n",
    "combined[\"school_dist\"] = combined[\"DBN\"].apply(get_first_two_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Find correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sat_score                               1.000000\n",
      "SAT Writing Avg. Score                  0.987771\n",
      "SAT Critical Reading Avg. Score         0.986820\n",
      "SAT Math Avg. Score                     0.972643\n",
      "frl_percent                             0.722225\n",
      "white_per                               0.620718\n",
      "asian_per                               0.570730\n",
      "AP Test Takers                          0.523140\n",
      "Total Exams Taken                       0.514333\n",
      "asian_num                               0.475445\n",
      "Number of Exams with scores 3 4 or 5    0.463245\n",
      "white_num                               0.449559\n",
      "sped_percent                            0.448170\n",
      "N_s                                     0.423463\n",
      "N_p                                     0.421530\n",
      "total_students                          0.407827\n",
      "ell_percent                             0.398750\n",
      "hispanic_per                            0.396985\n",
      "NUMBER OF STUDENTS / SEATS FILLED       0.394626\n",
      "female_num                              0.388631\n",
      "Name: sat_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlations = combined.corr()\n",
    "correlations = correlations[\"sat_score\"].abs()\n",
    "print(correlations.sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Taking the absolute value of coefficient, we can get a quick snapshot of how each value correlates with SAT scores. Keep in mind that some of these may be negatively correlated since we took the absolute value. Some quick notes:\n",
    " - Scores on the individual portions correlate highly, but is not useful to us since the individual portions make up the total SAT score\n",
    " - A high number of students will naturally lead to higher scores due to more chances for a high scores to occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Plotting survey correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove DBN since it's a unique identifier, not a useful numerical value for correlation.\n",
    "survey_fields.remove(\"DBN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rr_s          0.232199\n",
       "rr_t         -0.023386\n",
       "rr_p          0.047925\n",
       "N_s           0.423463\n",
       "N_t           0.291463\n",
       "N_p           0.421530\n",
       "saf_p_11      0.122913\n",
       "com_p_11     -0.115073\n",
       "eng_p_11      0.020254\n",
       "aca_p_11      0.035155\n",
       "saf_t_11      0.313810\n",
       "com_t_11      0.082419\n",
       "eng_t_11      0.036906\n",
       "aca_t_11      0.132348\n",
       "saf_s_11      0.337639\n",
       "com_s_11      0.187370\n",
       "eng_s_11      0.213822\n",
       "aca_s_11      0.339435\n",
       "saf_tot_11    0.318753\n",
       "com_tot_11    0.077310\n",
       "eng_tot_11    0.100102\n",
       "aca_tot_11    0.190966\n",
       "Name: sat_score, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "combined.corr()[\"sat_score\"][survey_fields] #.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-448c9dcd472d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's focus on the correlations between SAT score and the survey fields.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# First, let's remove DBN since it's a unique identifier, which is not useful for correlation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msurvey_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DBN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "# Let's focus on the correlations between SAT score and the survey fields.\n",
    "# First, let's remove DBN since it's a unique identifier, which is not useful for correlation.\n",
    "survey_fields.remove(\"DBN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_fields.remove(\"DBN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = combined.corr()\n",
    "correlations = correlations[\"sat_score\"].abs()\n",
    "print(correlations.sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "combined.corr()[\"sat_score\"][survey_fields] #.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_fields_corr = pd.DataFrame(combined.corr()['sat_score'][survey_fields]).reset_index()\n",
    "survey_fields_corr = survey_fields_corr.rename(columns={'index': 'field', 'sat_score':'sat_score_corr'})\n",
    "alt.Chart(survey_fields_corr).mark_bar().encode(\n",
    "    x='field',\n",
    "    y='sat_score_corr'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_fields_corr = pd.DataFrame(combined.corr()['sat_score'][survey_fields]).reset_index()\n",
    "survey_fields_corr = survey_fields_corr.rename(columns={'index': 'field', 'sat_score':'sat_score_corr'})\n",
    "alt.Chart(survey_fields_corr).mark_bar().encode(\n",
    "    x='field',\n",
    "    y='sat_score_corr'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
